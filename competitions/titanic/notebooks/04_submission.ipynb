{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üì§ Submission Generation\n",
                "\n",
                "> Competition: {{ COMPETITION_NAME }}\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì¶ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('../../..')\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import joblib\n",
                "\n",
                "from shared.utils import set_seed\n",
                "from shared.evaluation import create_submission\n",
                "\n",
                "set_seed(42)\n",
                "\n",
                "%load_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚öôÔ∏è Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "PROCESSED_PATH = 'data/processed'\n",
                "MODELS_PATH = 'models'\n",
                "SUBMISSIONS_PATH = 'submissions'\n",
                "\n",
                "TARGET_COL = 'target'  # Update this\n",
                "ID_COL = 'id'  # Update this\n",
                "TASK = 'classification'  # or 'regression'\n",
                "N_FOLDS = 5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìÇ Load Test Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test = pd.read_csv(f'{PROCESSED_PATH}/test_processed.csv')\n",
                "print(f\"Test shape: {test.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load sample submission for reference\n",
                "sample_sub = pd.read_csv('data/raw/sample_submission.csv')\n",
                "print(f\"Submission columns: {sample_sub.columns.tolist()}\")\n",
                "sample_sub.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ü§ñ Load Models & Generate Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature columns (same as training)\n",
                "EXCLUDE_COLS = [ID_COL]\n",
                "FEATURE_COLS = [c for c in test.columns if c not in EXCLUDE_COLS]\n",
                "X_test = test[FEATURE_COLS]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LightGBM predictions\n",
                "lgb_preds = np.zeros(len(test))\n",
                "for fold in range(N_FOLDS):\n",
                "    model = joblib.load(f'{MODELS_PATH}/lgb_fold{fold}.pkl')\n",
                "    lgb_preds += model.predict_proba(X_test) / N_FOLDS\n",
                "\n",
                "print(f\"LightGBM predictions: mean={lgb_preds.mean():.4f}, std={lgb_preds.std():.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# XGBoost predictions\n",
                "xgb_preds = np.zeros(len(test))\n",
                "for fold in range(N_FOLDS):\n",
                "    model = joblib.load(f'{MODELS_PATH}/xgb_fold{fold}.pkl')\n",
                "    xgb_preds += model.predict_proba(X_test) / N_FOLDS\n",
                "\n",
                "print(f\"XGBoost predictions: mean={xgb_preds.mean():.4f}, std={xgb_preds.std():.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ensemble predictions\n",
                "final_preds = (lgb_preds + xgb_preds) / 2\n",
                "print(f\"Ensemble predictions: mean={final_preds.mean():.4f}, std={final_preds.std():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Sanity Checks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "axes[0].hist(lgb_preds, bins=50, alpha=0.7, label='LGB')\n",
                "axes[0].set_title('LightGBM Predictions')\n",
                "\n",
                "axes[1].hist(xgb_preds, bins=50, alpha=0.7, label='XGB', color='orange')\n",
                "axes[1].set_title('XGBoost Predictions')\n",
                "\n",
                "axes[2].hist(final_preds, bins=50, alpha=0.7, label='Ensemble', color='green')\n",
                "axes[2].set_title('Ensemble Predictions')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üíæ Generate Submission"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from datetime import datetime\n",
                "\n",
                "os.makedirs(SUBMISSIONS_PATH, exist_ok=True)\n",
                "\n",
                "# Create timestamp for versioning\n",
                "timestamp = datetime.now().strftime('%Y%m%d_%H%M')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create submission\n",
                "test_ids = test[ID_COL] if ID_COL in test.columns else range(len(test))\n",
                "\n",
                "submission = create_submission(\n",
                "    test_ids=test_ids,\n",
                "    predictions=final_preds,\n",
                "    id_col=ID_COL,\n",
                "    target_col=TARGET_COL,\n",
                "    filename=f'{SUBMISSIONS_PATH}/submission_{timestamp}.csv',\n",
                "    threshold=0.5 if TASK == 'classification' else None\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify against sample submission\n",
                "assert len(submission) == len(sample_sub), \"Submission length mismatch!\"\n",
                "print(\"‚úÖ Submission validated!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üöÄ Submit to Kaggle (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment to submit via Kaggle API\n",
                "# !kaggle competitions submit -c COMPETITION_SLUG -f {SUBMISSIONS_PATH}/submission_{timestamp}.csv -m \"Ensemble LGB+XGB\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "**Done!** üéâ Good luck on the leaderboard!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}