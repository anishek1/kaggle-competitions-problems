{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ¤– Model Training & Validation\n",
                "\n",
                "> Competition: {{ COMPETITION_NAME }}\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“¦ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('../../..')\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from shared.utils import set_seed\n",
                "from shared.models import LGBMWrapper, XGBWrapper, cross_validate\n",
                "from shared.evaluation import classification_report, regression_report, print_metrics, oof_score\n",
                "from shared.visualization import plot_feature_importance\n",
                "\n",
                "set_seed(42)\n",
                "\n",
                "%matplotlib inline\n",
                "%load_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## âš™ï¸ Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "PROCESSED_PATH = 'data/processed'\n",
                "TARGET_COL = 'target'  # Update this\n",
                "ID_COL = 'id'  # Update this\n",
                "TASK = 'classification'  # or 'regression'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“‚ Load Processed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train = pd.read_csv(f'{PROCESSED_PATH}/train_processed.csv')\n",
                "test = pd.read_csv(f'{PROCESSED_PATH}/test_processed.csv')\n",
                "\n",
                "print(f\"Train shape: {train.shape}\")\n",
                "print(f\"Test shape: {test.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare features\n",
                "EXCLUDE_COLS = [TARGET_COL, 'fold', ID_COL]\n",
                "FEATURE_COLS = [c for c in train.columns if c not in EXCLUDE_COLS]\n",
                "\n",
                "X = train[FEATURE_COLS]\n",
                "y = train[TARGET_COL]\n",
                "folds = train['fold']\n",
                "X_test = test[FEATURE_COLS]\n",
                "\n",
                "print(f\"Features: {len(FEATURE_COLS)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸš€ Model Training"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### LightGBM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lgb_params = {\n",
                "    'objective': 'binary' if TASK == 'classification' else 'regression',\n",
                "    'metric': 'auc' if TASK == 'classification' else 'rmse',\n",
                "    'num_leaves': 31,\n",
                "    'learning_rate': 0.05,\n",
                "    'feature_fraction': 0.8,\n",
                "    'bagging_fraction': 0.8,\n",
                "    'bagging_freq': 5,\n",
                "    'verbose': -1,\n",
                "    'seed': 42\n",
                "}\n",
                "\n",
                "oof_lgb, lgb_models = cross_validate(\n",
                "    LGBMWrapper,\n",
                "    X, y, folds,\n",
                "    params=lgb_params,\n",
                "    task=TASK,\n",
                "    num_boost_round=1000,\n",
                "    early_stopping_rounds=50\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate\n",
                "if TASK == 'classification':\n",
                "    metrics = classification_report(y, (oof_lgb > 0.5).astype(int), oof_lgb)\n",
                "else:\n",
                "    metrics = regression_report(y, oof_lgb)\n",
                "\n",
                "print_metrics(metrics, 'LightGBM OOF Results')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature importance\n",
                "importance_df = lgb_models[0].get_feature_importance()\n",
                "plot_feature_importance(importance_df, n_features=20)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### XGBoost"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "xgb_params = {\n",
                "    'objective': 'binary:logistic' if TASK == 'classification' else 'reg:squarederror',\n",
                "    'eval_metric': 'auc' if TASK == 'classification' else 'rmse',\n",
                "    'max_depth': 6,\n",
                "    'learning_rate': 0.05,\n",
                "    'subsample': 0.8,\n",
                "    'colsample_bytree': 0.8,\n",
                "    'seed': 42\n",
                "}\n",
                "\n",
                "oof_xgb, xgb_models = cross_validate(\n",
                "    XGBWrapper,\n",
                "    X, y, folds,\n",
                "    params=xgb_params,\n",
                "    task=TASK,\n",
                "    num_boost_round=1000,\n",
                "    early_stopping_rounds=50\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if TASK == 'classification':\n",
                "    metrics = classification_report(y, (oof_xgb > 0.5).astype(int), oof_xgb)\n",
                "else:\n",
                "    metrics = regression_report(y, oof_xgb)\n",
                "\n",
                "print_metrics(metrics, 'XGBoost OOF Results')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ¯ Ensemble"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple averaging\n",
                "oof_ensemble = (oof_lgb + oof_xgb) / 2\n",
                "\n",
                "if TASK == 'classification':\n",
                "    metrics = classification_report(y, (oof_ensemble > 0.5).astype(int), oof_ensemble)\n",
                "else:\n",
                "    metrics = regression_report(y, oof_ensemble)\n",
                "\n",
                "print_metrics(metrics, 'Ensemble OOF Results')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ’¾ Save Models & OOF Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.makedirs('../models', exist_ok=True)\n",
                "\n",
                "# Save models\n",
                "for i, model in enumerate(lgb_models):\n",
                "    model.save(f'../models/lgb_fold{i}.pkl')\n",
                "\n",
                "for i, model in enumerate(xgb_models):\n",
                "    model.save(f'../models/xgb_fold{i}.pkl')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save OOF predictions\n",
                "oof_df = pd.DataFrame({\n",
                "    ID_COL: train[ID_COL] if ID_COL in train.columns else range(len(train)),\n",
                "    'oof_lgb': oof_lgb,\n",
                "    'oof_xgb': oof_xgb,\n",
                "    'oof_ensemble': oof_ensemble,\n",
                "    'target': y\n",
                "})\n",
                "oof_df.to_csv(f'{PROCESSED_PATH}/oof_predictions.csv', index=False)\n",
                "print(\"ðŸ’¾ OOF predictions saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "**Next Steps**: Proceed to `04_submission.ipynb`"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}